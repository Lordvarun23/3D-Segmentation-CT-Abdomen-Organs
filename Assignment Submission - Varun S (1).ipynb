{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8cc79789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nibabel as nib\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c2d707",
   "metadata": {},
   "source": [
    "## Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6055215f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedicalImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, target_shape=(512, 512, 100), transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.target_shape = target_shape\n",
    "\n",
    "        self.image_files = sorted([f for f in os.listdir(image_dir) if f.endswith('.nii.gz')])\n",
    "        self.label_files = sorted([f for f in os.listdir(label_dir) if f.endswith('.nii.gz')])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def resize_volume(self,volume, target_shape):\n",
    "        volume = torch.tensor(volume, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "        volume = F.interpolate(volume, size=target_shape, mode='trilinear', align_corners=False)\n",
    "\n",
    "        volume = volume.squeeze(0)\n",
    "\n",
    "        return volume\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        label_path = os.path.join(self.label_dir, self.label_files[idx])\n",
    "\n",
    "        image = nib.load(image_path).get_fdata()\n",
    "        label = nib.load(label_path).get_fdata()\n",
    "\n",
    "        image = self.resize_volume(image,(128, 128, 128))\n",
    "        label = self.resize_volume(label,(128, 128, 128))\n",
    "\n",
    "        label = label.squeeze(0).long()  \n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "29832259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 1, 128, 128, 128])\n",
      "torch.Size([1, 128, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "image_dir = 'C:\\\\Projects\\\\Hackathons\\\\5C Fellowship\\\\FLARE22Train\\\\FLARE22Train\\\\images'\n",
    "label_dir = 'C:\\\\Projects\\\\Hackathons\\\\5C Fellowship\\\\FLARE22Train\\\\FLARE22Train\\\\labels'\n",
    "dataset = MedicalImageDataset(image_dir, label_dir)\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "for images, labels in data_loader:\n",
    "    print(images.shape)  \n",
    "    print(labels.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "50715086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels in FLARE22_Tr_0001.nii.gz: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "Unique labels in FLARE22_Tr_0002.nii.gz: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "Unique labels in FLARE22_Tr_0003.nii.gz: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "Unique labels in FLARE22_Tr_0004.nii.gz: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "Unique labels in FLARE22_Tr_0005.nii.gz: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "Unique labels in FLARE22_Tr_0006.nii.gz: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "Unique labels in FLARE22_Tr_0007.nii.gz: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "Unique labels in FLARE22_Tr_0008.nii.gz: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "Unique labels in FLARE22_Tr_0009.nii.gz: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "Unique labels in FLARE22_Tr_0010.nii.gz: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "Unique labels in FLARE22_Tr_0011.nii.gz: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "Unique labels in FLARE22_Tr_0012.nii.gz: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "Unique labels in FLARE22_Tr_0013.nii.gz: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "Unique labels in FLARE22_Tr_0014.nii.gz: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "Unique labels in FLARE22_Tr_0015.nii.gz: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "Unique labels in FLARE22_Tr_0016.nii.gz: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "Unique labels in FLARE22_Tr_0017.nii.gz: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "Unique labels in FLARE22_Tr_0018.nii.gz: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "Unique labels in FLARE22_Tr_0019.nii.gz: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "Unique labels in FLARE22_Tr_0020.nii.gz: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "Unique labels in FLARE22_Tr_0021.nii.gz: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "Unique labels in FLARE22_Tr_0022.nii.gz: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "Unique labels in FLARE22_Tr_0023.nii.gz: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "Unique labels in FLARE22_Tr_0024.nii.gz: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "Unique labels in FLARE22_Tr_0025.nii.gz: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "Unique labels in FLARE22_Tr_0026.nii.gz: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "Unique labels in FLARE22_Tr_0027.nii.gz: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "Unique labels in FLARE22_Tr_0028.nii.gz: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "Unique labels in FLARE22_Tr_0029.nii.gz: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "Unique labels in FLARE22_Tr_0030.nii.gz: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "Unique labels in FLARE22_Tr_0031.nii.gz: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "Unique labels in FLARE22_Tr_0032.nii.gz: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "Unique labels in FLARE22_Tr_0033.nii.gz: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "Unique labels in FLARE22_Tr_0034.nii.gz: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "Unique labels in FLARE22_Tr_0035.nii.gz: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "Unique labels in FLARE22_Tr_0036.nii.gz: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "Unique labels in FLARE22_Tr_0037.nii.gz: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "Unique labels in FLARE22_Tr_0038.nii.gz: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "Unique labels in FLARE22_Tr_0039.nii.gz: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "Unique labels in FLARE22_Tr_0040.nii.gz: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "Unique labels in FLARE22_Tr_0041.nii.gz: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "Unique labels in FLARE22_Tr_0042.nii.gz: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "Unique labels in FLARE22_Tr_0043.nii.gz: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "Unique labels in FLARE22_Tr_0044.nii.gz: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "Unique labels in FLARE22_Tr_0045.nii.gz: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "Unique labels in FLARE22_Tr_0046.nii.gz: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "Unique labels in FLARE22_Tr_0047.nii.gz: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "Unique labels in FLARE22_Tr_0048.nii.gz: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "Unique labels in FLARE22_Tr_0049.nii.gz: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "Unique labels in FLARE22_Tr_0050.nii.gz: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "Total unique labels across all files: [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0]\n"
     ]
    }
   ],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def check_unique_labels(label_file):\n",
    "    label_data = nib.load(label_file).get_fdata()\n",
    "    \n",
    "    unique_labels = np.unique(label_data)\n",
    "    \n",
    "    return unique_labels\n",
    "\n",
    "\n",
    "label_files = sorted([os.path.join(label_dir, f) for f in os.listdir(label_dir) if f.endswith('.nii.gz')])\n",
    "\n",
    "for label_file in label_files:\n",
    "    unique_labels = check_unique_labels(label_file)\n",
    "    print(f\"Unique labels in {os.path.basename(label_file)}: {unique_labels}\")\n",
    "\n",
    "all_unique_labels = set()\n",
    "for label_file in label_files:\n",
    "    unique_labels = check_unique_labels(label_file)\n",
    "    all_unique_labels.update(unique_labels)\n",
    "\n",
    "print(f\"Total unique labels across all files: {sorted(all_unique_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a46a8e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MedicalImageDataset(image_dir, label_dir)\n",
    "train_loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "val_dataset = MedicalImageDataset(image_dir, label_dir)\n",
    "val_loader = DataLoader(dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dc2658d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_volume(volume):\n",
    "    min_val = np.min(volume)\n",
    "    max_val = np.max(volume)\n",
    "    volume = (volume - min_val) / (max_val - min_val)\n",
    "    return volume\n",
    "\n",
    "def resize_volume(volume, new_shape=(128, 128, 128)):\n",
    "    resampler = sitk.ResampleImageFilter()\n",
    "    resampler.SetSize(new_shape)\n",
    "    resampler.SetOutputSpacing([osz * ospc / nsz for osz, ospc, nsz in zip(volume.GetSize(), volume.GetSpacing(), new_shape)])\n",
    "    resampler.SetInterpolator(sitk.sitkLinear)\n",
    "    return sitk.GetArrayFromImage(resampler.Execute(volume))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a74d5f",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "250f43eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class VNet(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(VNet, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv3d(1, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv3d(32, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(16, num_classes, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Upsample(scale_factor=2, mode='trilinear', align_corners=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "model = VNet(num_classes=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "effafb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def dice_loss(pred, target, smooth=1.0):\n",
    "    intersection = (pred * target).sum()\n",
    "    return 1 - ((2. * intersection + smooth) / (pred.sum() + target.sum() + smooth))\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5208871c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_score = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss, model, epoch):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = val_loss\n",
    "            self.save_checkpoint(val_loss, model, epoch)\n",
    "        elif val_loss > self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = val_loss\n",
    "            self.save_checkpoint(val_loss, model, epoch)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model, epoch):\n",
    "        \"\"\"Saves model when validation loss decreases.\"\"\"\n",
    "        print(f\"Validation loss decreased ({self.best_score:.6f} --> {val_loss:.6f}). Saving model...\")\n",
    "        torch.save(model.state_dict(), f'best_model_epoch_{epoch}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "324a32f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelCheckpoint:\n",
    "    def __init__(self, save_path='best_model.pth', monitor='val_loss', mode='min'):\n",
    "        self.save_path = save_path\n",
    "        self.monitor = monitor\n",
    "        self.mode = mode\n",
    "        self.best_value = None\n",
    "\n",
    "    def __call__(self, value, model):\n",
    "        if self.best_value is None:\n",
    "            self.best_value = value\n",
    "            self.save_model(model)\n",
    "        elif (self.mode == 'min' and value < self.best_value) or (self.mode == 'max' and value > self.best_value):\n",
    "            self.best_value = value\n",
    "            self.save_model(model)\n",
    "\n",
    "    def save_model(self, model):\n",
    "        print(f\"Saving best model with {self.monitor}: {self.best_value:.6f}\")\n",
    "        torch.save(model.state_dict(), self.save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9e8a13",
   "metadata": {},
   "source": [
    "## Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "751c84b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 2.7938, Val Dice: -0.3406\n",
      "Validation loss decreased (-0.340639 --> -0.340639). Saving model...\n",
      "Saving best model with val_dice_score: -0.340639\n",
      "Epoch 2/5, Loss: 2.3552, Val Dice: -0.6728\n",
      "Validation loss decreased (-0.672837 --> -0.672837). Saving model...\n",
      "Epoch 3/5, Loss: 1.5974, Val Dice: -0.3970\n",
      "Epoch 4/5, Loss: 1.3746, Val Dice: 0.2439\n",
      "Saving best model with val_dice_score: 0.243891\n",
      "Epoch 5/5, Loss: 1.5423, Val Dice: -0.2383\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(patience=10, min_delta=0.001)\n",
    "model_checkpoint = ModelCheckpoint(save_path='best_model.pth', monitor='val_dice_score', mode='max')\n",
    "\n",
    "num_epochs=5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training step\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, masks in train_loader:\n",
    "        images = images\n",
    "        masks = masks\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks) + dice_loss(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    \n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    val_dice_scores = []\n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            images = images\n",
    "            masks = masks\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            dice_score = dice_loss(preds.float(), masks.float()).item()\n",
    "            val_dice_scores.append(dice_score)\n",
    "    \n",
    "    avg_val_dice = np.mean(val_dice_scores)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Val Dice: {avg_val_dice:.4f}\")\n",
    "\n",
    "    # Callbacks\n",
    "    early_stopping(avg_val_dice, model, epoch)\n",
    "    model_checkpoint(avg_val_dice, model)\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679e8154",
   "metadata": {},
   "source": [
    "## Inference Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d02a943",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VNet(num_classes=14)  \n",
    "\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61ea622",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch\n",
    "\n",
    "def predict_segmentation(model, input_image):\n",
    "    model.eval()  \n",
    "    with torch.no_grad(): \n",
    "        input_tensor = torch.tensor(input_image).unsqueeze(0).unsqueeze(0).float()  \n",
    "\n",
    "        output = model(input_tensor) \n",
    "\n",
    "        probabilities = torch.softmax(output, dim=1)\n",
    "        predicted_segmentation = torch.argmax(probabilities, dim=1)  # Shape: [1, 128, 128, 128]\n",
    "        predicted_segmentation = predicted_segmentation.squeeze(0).cpu().numpy().astype(np.int32)  # Shape: [128, 128, 128]\n",
    "\n",
    "    return predicted_segmentation\n",
    "\n",
    "\n",
    "input_image = nib.load('C:\\\\Projects\\\\Hackathons\\\\5C Fellowship\\\\FLARE22Train\\\\FLARE22Train\\\\images\\\\FLARE22_Tr_0002_0000.nii.gz').get_fdata()\n",
    "predicted_segmentation = predict_segmentation(model, input_image)\n",
    "nib.save(nib.Nifti1Image(predicted_segmentation, np.eye(4)), 'predicted_segmentation.nii.gz')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
